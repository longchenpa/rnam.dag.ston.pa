<html>
<head>
    <meta charset="utf-8" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="" />
    <meta name="author" content="Maxim Sokhatsky" />
    <title>2020-09-02 Коллекция компьютеров</title>
    <link rel="stylesheet" href="https://n2o.dev/blank.css" />
    <link rel="stylesheet" href="../../journal.css" />
</head>
<body>
<nav>
    <a href='../../index.html'>5HT</a>
    <a href='../index.html'>TOP</a>
    <a href='#'>2020-09-02</a>
</nav>
<main>
    <section>
        <h3>Коллекция компьютеров</h3>

        <h4>О чем</h4>

        <p>Даный текст &mdash; это многоцелевая попытка рассказать историю
           компьтерной техники: задокументировать
           развитие и выделить генеалогию, попытаться рассказать о компьютере
           с точки зрения потребительской электроники, сформировать список
           лучших коллекционных экземпляров для строительства шоурумов
           или ютуб каналов. В качестве примера строится сборка Quadstellar
           на платформе X266 плате ASUS RAMPAGE VI.</p>

        <h4>История написания</h4>

        <p>Этот текст появился благодаря носталигии за PC культурой
           и возвращении к ее DIY истокам. Теперь, когда пришла старость,
           могу себе позволить воссоздать коллекцию компьютеров которыми
           владел в этой жизни. Авторский период 2010-2018 годов,
           который требовал только печатную машинку и процессор уровня ARM32, давно позади и
           теперь хочется занятся настоящими вычислительными задачами,
           которые требуют ресурсов уровня рабочих станций. В основном,
           я планирую заняться системами хранения на SSD/NVMe с использованием spdk и RocksDB/NVMe,
           а для строительства этих масивов нужны желательно невиртуальные диски,
           и побольше, ведь речь идет о 32 линиях PCI Express (PCIe) для дисковой подсистемы.
           Поэтому денег это все будет потреблять много, особенно с учетом желания
           работать на современном железе версии PCIe 4.0. Также хочу поэкспериментировать
           с системами индексации, которые работают в пространстве GPU (такими как Uber AresDB).
           Для этих целей нужны видеокарты с максимальным количеством памяти,
           а это обычно карты уровня Titan. К счастью, nVidia с анонсом 30-й серии снизила
           цены на такие карты в три раза (до $1500 за 24ГБ). Начиная с планов покупки этих современных
           дорогостоящих компонент я плавно войду в рестроспективу компьютерной
           техники (в том числе и свою личную)
           и прослежу историю ее развития с точки зрения стандарта PCIe,
           первая версия которого была создана в 2003 году. Все что до PCIe 1.0 не представляет
           для меня коллекционной ценности.</p>

       <p>Обычно, на курсах системного программирования изучается какой-то булшит,
          какие-то несуществующие MIX процессоры, все это решительно я порицаю!
          Присоединяйтесь к унификации промышленности и академии в виде таких 
          проектов как RISC-V, который тоже компонуется на шине PCI Express на примере
          таких плат как HiFive Unleashed Expansion Board со слотом x16 и
          своим PCIe свичем для периферии. PCIe также можно найти в
          легендарной октрытой платформе IBM POWER9, представленой
          такими материнскими платами как Talos II (PCIe 4.0 между прочим).
          Gigabyte давно уже делает ARM64 на платформе Marvel ThunderX2, тоже на PCIe.
          Я же считаю, что каждый системный программист должен понимать как устроен
          компьютер и как программируется PCI Express. Как минимум spdk &mdash; это
          прикладного уровня фреймворк, поэтому извинения не принимаются!</p>

       <h4>Что такое компьютер</h4>

        <p>Для начала хочу рассказать, что такое компьютер, с точки зрения каких
           критериев мы его представляем в этом выпуске.
           Компьютер &mdash; это система из следующих устройств:
           1) процессор;
           2) память;
           3) видеокарта;
           4) диск;
           5) сетевая карта;
           6) система питания и другие технические модули;
           которые соединены определенным образом (обычно это шина или
           непосредственное соединение точка-точка чере хаб (InfiniBand, PCIe, QPI, HyperTransport, etc).
           Эти соединения представляют собой видимые дорожки на материнской плате,
           где компонуется вся электроника. В случае PCIe такой хаб называется корневой комплекс,
           куда сходятся все линии PCIe. Основной принцип DYI электроники &mdash; это
           максимально гибкая система расширений, позволяющая добавлять дополнительные устройства
           в виде плат-расширений в специальные места которые подключены к шине или к хабу.</p>

        <figure><figcaption>Cоединения компонент в типичном PC</figcaption>
                <img src="img/PCIe.png?v=2" height=380></figure>

        <p>Виды таких систем соединения компонент называются платформы. Различают
           электрические (PCI, PCIe) и форм-факторные спецификации (PC/104, PCIe, mPCIe) платформ.
           PCIe вообще стала народной шиной, которая стала де-факто стандартом и при строительстве
           датацентров и на рынке потребителькой электроники, вытеснив такие, ранее казавшись
           перспективныыми, стандарты как InfiniBand или RapidIO. Теперь PCIe используется
           не только как соединение для массовых (Intel или AMD),
           но закрепилась и для более открытых (POWER9, ARM, RISC-V) микропроцессорных архиитектур.</p>

       <h4>Причины успеха PCIe</h4>

       <p>Причины такой популярности в действительно хорошей архитектуре электрической спецификации PCIe.
          В отличие от шины PCI, которая работает в общем случае как аппаратный брокер каналов,
          PCIe представляет собой систему из корневого комплекса и набора свичей которые можно подключать
          к нему каскадами. Эти устройства можно настроить таким образом, чтобы они передавали
          даные друг другу без участия посредников запрограммировав определенным образом
          роутинг корневого комплекса и системы PCIe свичей. Будучи сконфигурироваными, устройства
          могут использовать общую память и синхронизировать передачу даных между своими буферами (DMA).
          Представьте себе, что когда копируется файл с диска на диск, CPU при
          этом не пересылает никакие байты. Приблизительно то же происходит когда вы подключили
          две видеокарты в SLI режим, они общаются между собой по PCIe шине и способны
          обмениваться данными не требуя отвлечения процессора. Теоретически возможно подключение
          и сетевых карт к любым PCIe устройствам которые совместимы по атрибутам подключения,
          так сетевая карта может складировать пакеты на диск автоматически без участия процессора,
          который возможно позже прочитает эти данные асинхронным образом. К сожелению
          архитектурные ограничения асинхронной природы TCP/IP не позволяют делать такие чудеса
          в промышленном окружении, для этого желательно полностью перепроектировать сетевой уровень.
          Благодаря этим особенностям, которые ранее были доступны только в более дорогих оптических
          стандартах типа InfiniBand, вместе с понятной и гибкой
          системой масштабирования которая увеличивает пропускную способность дважды каждую версию,
          стандарт PCI Express продержался на рынке 17 лет и еще продержиться до момента схлопывания
          закона Мура в 2024 году (оринетировочно PCIe 7.0), когда мы достигнем 2нм техпроцесса
          с использованием нанотрубок.</p>

       <h4>Проблемы масштабирования PCIe в потребительской электронике</h4>

       <p>С помощью PCIe свичей распаивается система ввода-вывода
          для подключения внешних устройств. Обычно это подключается к проприетарному
          мосту и собственному протоколу, который соединяет корневой комплекс
          с системой свичей для внешних устройств. В случае Intel такой протокол называется DMI
          и имеет архитектурное ограничение в 4 PCIe линии и это не дает возможности
          строить эффективные RAID NVMe массивы на Intel платформе, которые обычно подключаются
          проприетарному мосту для внешних устройств через "бутылочное горлышко" DMI,
          которое, в свою очередь, подключено к корневому комплексу &mdash; это первая
          проблема масштабирования.</p>

       <p>К счатью на материнских платах распаиваются слоты для видеокарт,
          которые подлючены не через DMI интерконнект, а напрямую к корневому комплексу
          через высокопроизводительный свич которые обычно
          виден как PCI-to-PCI мост. Вы можете использовтаь эти слоты для строительства
          дискового массива, но вы тогда лишаетесь возможности использовать GPU.</p>

       <p>Другая проблема которая может возникнуть при масштабировании дискового массива &mdash;
          это количество PCIe линий, которые можно подключить к корневому комплексу,
          которые в современных системах находится прямо в процессоре.</p>

       <p>И наконец существует третья проблема &mdash; это количество ядер в процессоре,
          если их очень мало, то каждое прерывание на линии будет требовать отвлечения процессора,
          что будет создавать много лишних преключений контекстов процессора и снижать
          общую теоретическую производительность. В идеальной синхронной системе,
          для контролья каждой PCIe линии нужно выделять отдельное ядро процессора, чтобы
          в случае прерывания возникшего при обмене данными между устройствами это
          никаким образом не влияло на работу остальных потоков системы (QoS на уровне линий PCIe).
          Таким образом хотелось бы процессор
          у которого ядер в два раза больше чем PCIe линий, потому как хочется же еще что-то считать.
          Это конечно жесткий оверкил, но только так можно достичь
          гарантий уровня систем реального времени. Именно такие эксперименты с дисковыми
          массивами меня и инетерсуют как продолжение <a href="https://github.com/o83/n2o">моего исследования</a> высокопроизводительных
          систем реального времени.
          </p>


       <h4>Структура коллекции</h4>

       <p>Свою коллекцию вижу как подрешетку нескольких измерений:
          1) форм-фактор материнской платы: Mini-ITX (компактные), EEB/EATX/XLATX (рабочие станции);
          2) тип микропроцессорной архитектуры: Intel, AMD, ARM64, POWER9, RISC-V;
          3) версия PCIe: 3.0, 4.0 (версии, которые поддерживают NVMe M.2).
          Первые коллекционные, рассматриваемые мной чипсеты, это первые чипсеты для PCIe,
          которые имели графическую шину 16 PCIe линий и были созданы для первых
          процессоров Intel Core.</p>

       <p>Intel Core ведет свою родословную от процессоров Intel Pentium Pro (P6).
          Поколения 32-битных x86 процессоров Intel:
          1) i386;
          2) i486, первые мои процессоры и первый Intel Datasheet который я прочитал
             от первой до последней страницы, у меня были и Intel и AMD чипы,
             а также AMD версия i586 с множителем FSB x5);
          3) Pentium (P5, 1994), которое вошло на рынок почти одновременно со своими серверными,
             более мощными аналогами P6;
          4) Pentium Pro (P6, 1995), Pentium II, Pentium III &mdash; эти процессоры
             имели общее архитектурное ядро, и развивались в основном в направлении
             мультимедийных расширений (MMX, SSE, которые стали прекурсорами
             современных AVX, NNI инструкций), экспериментах с сокетами и картриджами.
             В это время появились процессоры Xeon, с увеличиным объемом кешей.
       </p>

       <p><b>Многосокетный период</b></p>

       <p>У меня были двухпроцессорных материнские платы для картриджных (TYAN Tiger 133, Slot 1)
          и сокетных (TYAN Thunder, Socket 370) P6 процессоров,
          коллекция материнских плат на чипсете i440BX (ABit BP6, ASUS P2B),
          две легендарные оверклокерские
          материнские платы ASUS CUSL2 и TUSL2. Что сказать,
          я упарывался по дуальным мамкам на Intel тогда. К сожалению, а может к счатью
          эта часть коллекции утеряна и, судя по ценам на эти платы на вторичном рынке
          с учетом низкокачественной технологии производства конденсаторов того времени,
          восстанавливать ее я не собираюсь.</p>

       <p>
          Пятая по счету архитектура 32-битных процессоров Intel &mdash; Pentium 4 NetBurst
          оказалась совершенно неудачной, хотели даже cменить стандарт на блоки
          питания (с ATX на BTX) из-за увеличеных показателей рассеиваемой мощности.
          Позже Intel отказалась от NetBurst архитектуры и за основу следующей архитектуры
          был взял P6. Следуюшая архитектура была уже 64-битной и получила название Intel Core, впервые появились VT-x, SSE3.
          Эта архитектура и живет в процессорах Intel и до сих пор и именно с процессоров
          Intel Core и чипсета Q35 начнется моя коллекция. Собирать будем
          комплектами по типу сокетов и чипсетов: материнская плата,
          максимальный процессор, максимальный альтернаьтивный процессор,
          максимальный объем ОЗУ.
       </p>

       <p><b>Компактные системы</b></p>

       <p>Компактными мы называем системы с количеством ядер от 4 до 16.
          Начиная с Intel Core стало понятно, что мосты, видеокарты, корневые
       комплексы, котроллеры памяти, когерентный кеш &mdash; все это лучше
       размещать прямо на кристале процессора. Многопроцессорные системы стали
       односокетными и популярной стала минитюаризация, рынок наполнился Mini-ITX
       платами и энергоэффетивность стала важнее производительности на пути прогресса.</p>

       <p>Чипсет долгожитель для платформы Intel является Z370/X299, указан как единая спецификация,
          потому, что X299 и Z370 в смысле южных мостов одинковы с точки зрения программирования,
           они предлагают
          следующую модель распределения своих 30 HSIO линий чипсета
          (24 из которых мультиплексируются через свичи и лишь 16
          из которых могут работать одноврменно): </p>

       <p><b>Z370/X299</b>. 1-6: USB, 7-9: USB/PCIe (3), 10: USB/PCIe/LAN (1), 11: PCIe/LAN (1), 12-14: PCIe (3),
          15-16: PCIe/SATA (2), 17: PCIe (1), 18: PCIe/LAN (1), 19: PCIe/LAN/SATA (1),
          20-24: PCIe/SATA (5), 25-26: PCIe (2), 27-30: PCIe RST (4).</p>

       <figure><img src="https://5ht.co/tristellar/177502_02_TriStella-2.jpg" height=280></figure>

       <P>Разница между X299 и Z370 лишь в количестве лининий, которыми может обладать процессор,
          и которые будут разведены на материнской плате к PCIe портам (желательно без свичей). Платформа X299
          позваляет подключать к процессору 3 полных x16 слота или 48 линий. Если вам нужны 4 полных
          x16 слота, то для этого нужно переходить в платформу LGA3647/C261, где Xeon-W имеют как раз 64 линии.
          Для полноценного паралельного линка четырех x16 карт нужно 64 линии PCIe или это 16 дисков NVMe
          на которых можно построить RAID-0 на четырех M.2 x16 картах. Стоимость процессора
          с 48 линиями будет $1K, а материнская плата от $350 (Mini-ATX) до $700.
          Строить RAID-0 средствами VROC может быть даже выгодней, потому, что вряд
          ли какая-то RAID-0 карточка может быть лучше чем топовые Intel процессоры
          с мегабайтными кешами.</p>

       <figure><table cellspacing=10>
       <thead>
       <th>Platform</th><th>x/PCIe/DMI</th><th>Motherboard</th><th>CPU</th><th>Alternative CPU</th>
       </thead>
       <tbody>
       <tr style="color: #330000; font-weight: bold;"><td>PGA1331/X570</td><td>24/4.0</td><td>mITX STRIX X570-I</td><td>Ryzen 9</td><td></td></tr>
       <tr style="color: #000033; font-weight: bold;"><td>LGA1151/Z370</td><td>24/3.0/3.0</td><td>mITX STRIX Z370-I</td><td>i7-8700K</td><td>i9-9900K</td></tr>
       <tr style="color: #000033; font-weight: bold;"><td>LGA2066/X299</td><td>24/3.0/3.0</td><td>mITX X299E-ITX/ac</td><td>i7-9800X</td><td>i9-10980XE</td></tr>
       <tr><td>LGA2011-v3/X99</td><td>8/3.0/2.0</td><td>mITX X99E-ITX/ac</td><td>i7-6950X</td><td>E5-4669 v4</td></tr>
       <tr><td>LGA1150/Z97</td><td>8/3.0/2.0</td><td>mITX MAXIMUS VII IMPACT</td><td>i7-4790K</td><td>E3-1286 v3</td></tr>
       <tr><td>LGA1155/Z77</td><td>8/3.0/2.0</td><td>mITX P8Z77-I DELUXE</td><td>i7-3770K</td><td>E3-1245 v2</td></tr>
       <!--tr><td>LGA1151/Z270</td><td>24/3.0/3.0</td><td>mITX STRIX Z270</td><td>i7-7700K</td><td></td></tr-->
       <!--tr><td>LGA775/Q35</td><td>1.0/1.0</td><td>ATX P5E-VM DO</td><td>Q9650</td><td>ED-965, P4-672, D-960</td></tr>
       <tr><td>LGA775/Q45</td><td>1.0/1.0</td><td>ATX P5Q-EM DO</td><td>Q9650</td><td>ED-965</td></tr-->
       </tbody></table></figure>

       <p>Несколько комментариев по поводу это таблицы. Здесь собраны шесть плат на пяти чипсетах,
          покрывающие потребительские процессоры AMD и Intel.
          Это такая игра: собрать минимальное количество материнских плат,
          в которые можно вставить любой процессор. После Z370 я перестал
          обновлятся, так как не считаю новые чипсеты достойные апгрейда,
          пока не выпустятся новые серверные аналоги чипсетов. Z390 отличается от Z370
          только поддержкой 64ГБ RAM и USB 3.2 портом дополнительным.
          Z490 просто смена сокета, хотя эти же процессоры 10-й серии
          можно получить на прошлой X299 платформе, да еще и более мощные
          с большим количеством PCIe линий. Вообщем вы как хотите, а
          я коллекциоными Z390 и Z490 не считаю, хотя от нового
          STRIX Z390-I не отказался бы для своего 9900K,
          чтобы запаковать его до 64GB RAM.</p>

       <figure><table cellspacing=10>
       <thead>
       <th>Codename</th><th>CPU</th><th>Cores</th><th>Lanes</th><th>Motherboard</th>
       </thead>
       <tbody>
       <tr><td>Broadwell</td><td>E5-2699V4</td><td>22</td><td>40</td><td>X99E-ITX/ac</td></tr>
       <tr><td>Haswell</td><td>E5-4669V3</td><td>18</td><td>40</td><td>X99E-ITX/ac</td></tr>
       <tr style="color: #330000; font-weight: bold;"><td>Zen 3</td><td>Ryzen 9</td><td>16</td><td>24</td><td>STRIX X570-I</td></tr>
       <tr style="color: #000033; font-weight: bold;"><td>Coffee Lake</td><td>i9-9900K</td><td>8</td><td>16</td><td>STRIX Z370-I</td></tr>
       <tr style="color: #000033; font-weight: bold;"><td>Coffee Lake</td><td>i7-8700</td><td>6</td><td>16</td><td>STRIX Z370-I</td></tr>
       <tr><td>Haswell</td><td>E3-1286V3</td><td>4</td><td>16</td><td>MAXIMUS VII IMPACT</td></tr>
       <tr><td>Ivy Bridge</td><td>E3-1245V2</td><td>4</td><td>16</td><td>P8Z77-I DELUXE</td></tr>
       </tbody></table></figure>

          <p>Что касается AMD, то к счастью она привела именование своих потребительских продуктов в
             соотвествии с линейкой Intel, модели Ryzen 3, 5, 7, 9.
             Это аналоги линейки Intel Core с количество ядер до 16.
             Все эти процессоры (Zen+, Zen 2, Zen 3) можно установить в mITX материнскую
             плату ASUS ROG STRIX X570-I c лучшим на рынке VRM. Вообще платы mITX
             лучше для разгона.</p>

         <p>Как видите в легендарную X99E-ITX/ac можно воткнуть Xeon процессоры с 18 и 22 ядрами.
            Любое использование процессора на платформе Mini-ITX,
            потребление которого больше 65W на мой взгляд несбалансировано
            и является предметом исключительно энтузиазма и спортивного интереса.
            Если мне понадобится что-то посчитать, я буду увеличивать не мегагерцы,
            а количество ядер, да до такой степень, чтобы плюс минус пара
            ядер была уже не важна. Именно поэтому в этом обзоре вообще не упоминается устройство
            тактового генератора, важного элемента любого компьютера.
            Тогда пришлось бы рассказать про устройство адаптивной SpeedStep технологии
            которая регулирует напряжением на ядрах, частотой интерконекта и памяти.
            Канальность в памяти это как RAID с дисками, чем их больше чем быстрее может быть
            пямять, поэтому в бенмарках выигрывают древние Xeon с четырехканальными,
            а в некоторых случаях шести-канальными контроллерами памяти. При сборке
            Mini-ITX системы я бы прежде всего уделил внимание энергопотреблению,
            а потом уже выжал максимальную мощность &mdash; это стимулирует покупать
            современное оборудование. Правилом хорошего тона для Mini-ITX систем
            считаются блоки питания 600W уровня Titanium, желательно безвентиляторные.
            Такие БП существуют, я пользуюсь БП Seasonic, которые стояли
            в первых IBM PC (который у меня тоже был). Все что выше 600W &mdash; это энтузиазм.
            Например использование 9900K в Mini-ITX &mdash; это энтузиазм.</p>

       <p><b>Рабочие станции</b></p>

       <p>Рабочими станциями мы называем системы с количеством ядер больше 16.
          На рынке серверных процессоров увеличение ядер шло еще более бодрыми темпами,
          для увеличивающегося количества PCIe линий, нужно было больше ядер, и это
          стало основным приоритетом для эволюции действительно многоядерных архитектур.
          Mac Pro на платформе LGA3647 (Xeon-W) были построены с ипользованием чипсета C261,
          поэтому если хочется максимально
          оригинальный Хакинтош, то DOMINUS EXTREME за $1800 звучит как разумный выбор :-)
          Я сосредоточил свой фокус на Альфа и Омеге материнских плат для рабочих станций от ASUS.</p>

       <figure><table cellspacing=10>
       <thead>
       <th>Platform</th><th>x/PCIe/DMI</th><th>Motherboard</th><th>CPU</th><th>Alternative CPU</th>
       </thead>
       <tbody>
       <tr style="color: #330000; font-weight: bold;"><td>LGA4094/TRX40</td><td>88/4.0</td><td>ZENITH II EXTREME ALPHA</td><td>3990X</td><td></td></tr>
       <tr><td>LGA3647/C621</td><td>64/3.0/3.0</td><td>DOMINUS EXTREME</td><td>W-3275</td><td>Platinum 8280</td></tr>
       <tr style="font-weight: bold;"><td>LGA2066/X299</td><td>48/3.0/3.0</td><td>RAMPAGE VI EXTREME OMEGA</td><td>i9-10980XE</td><td></td></tr>
       </tbody></table></figure>

        <p>LGA3647 платорфма предназначена для Xeon процессоров семейств W и Platinum,
          которые ведут свою родословную не от Pentium Pro (P6), а от многоядерных Xeon Phi,
          где каждое ядро это Atom процессор, представленный ранее как энергоэффективный
          x86 конкурент ARM процессора.</p>

       <figure><table cellspacing=10>
       <thead>
       <th>Codename</th><th>CPU</th><th>Cores</th><th>Lanes</th><th>Motherboard</th>
       </thead>
       <tbody>
       <tr style="color: #330000; font-weight: bold;"><td>Zen 3</td><td>3390X</td><td>64</td><td>88</td><td>ZENITH II EXTREME ALPHA</td></tr>
       <!--tr><td>Cascade Lake</td><td>Xeon Platinum 9200</td><td>56</td><td>40</td></tr-->
       <tr><td>Cascade Lake</td><td>Xeon Platinum 8280</td><td>28</td><td>48</td><td>DOMINUS EXTREME</td></tr>
       <tr><td>Cascade Lake</td><td>W-3275</td><td>28</td><td>64</td><td>DOMINUS EXTREME</td></tr>
       <tr><td>Skylake</td><td>Xeon Platinum 8180</td><td>28</td><td>48</td><td>DOMINUS EXTREME</td></tr>
       <tr><td>Skylake</td><td>W-3175X</td><td>28</td><td>48</td><td>DOMINUS EXTREME</td></tr>
       <tr style="color: #000033; font-weight: bold;"><td>Cascade Lake</td><td>i9-10980XE</td><td>18</td><td>48</td><td>RAMPAGE VI EXTREME</td></tr>
       <tr style="color: #000033; font-weight: bold;"><td>Skylake</td><td>i9-9800X</td><td>8</td><td>44</td><td>RAMPAGE VI EXTREME</td></tr>
       <!--tr style="color: #000033; font-weight: bold;"><td>Coffe Lake</td><td>i9-9900K</td><td>8</td><td>16</td><td>mITX STRING Z370-I</td></tr>
       <tr style="color: #000033; font-weight: bold;"><td>Coffe Lake</td><td>i7-8700</td><td>6</td><td>16</td><td>mITX STRING Z370-I</td></tr-->
       </tbody></table></figure>

       <P>AMD начала выпускать энергоэффективные
          многоядерные (от 32) процессоры на Zen 3 архитектуре только через 7 лет (в 2017) после
          появления Xeon Phi процессоров. Зато сразу дали 88 PCIe линий,
          версию 4.0 и без проприетарном моста (DMI), почти все линии заведены
          сразу на корневой комплекс.</p>

        <figure><figcaption>Сравнение размеров Intel Core и Xeon Phi</figcaption>
                <img src="img/core-phi.png" height=280></figure>

       <p>Intel пришлось сделать ответный шаг на Threadripper и выпустить 10-е поколение Cascade Lake
          для платформы X299, чтобы продлить свой цейтнот. Я думаю специально для этой
          эмиссии 10-го поколение X-серии ASUS выпустила дополнительную серию плат RAMPAGE VI EXTREME OMEGA,
          подчеркивая момент уходящей платформы.</p>

       <p><b>Рабочая станция на платформе X299</b></p>

       <p>В качестве платформы была выбрана серия ASUS Rampage VI.
          На примере сериии материнских плат Rampage VI для X-Series процессоров Intel покажем
          как производители компонуют PCIe линии.</p>

       <p>Первая плата Rampage VI Apex была представлена на COMPUTEX в 2017 году.
          Через пол года вышла версия с U.2 портом Rampage VI Extreme.
          Через два года, на CES в 2019 была представлена плата Rampage VI Omega
          с более минималистичной PCIe разводкой ориентированной на NVMe массивы.
          Так же через пол года на GAMESCOM в 2019 году была представлена
          обновленная версия платы без U.2 порта Rampage VI Extreme Encore.
          Я самостоятельно восстановил конфигурации этих материнских плат
          для процессоров с 48 PCIe линиями, использовал при этом
          их руководства пользователя.</p>

        <p><b>RAMPAGE VI APEX Q1 2017</b></p>

        <p>Достаточно классическая и минималистичная конфигурация с двумя полными x16 слотами
           и еще одним дисковым массивом для VROC на DIMM.2 слоте на два NVMe x4. И еще остается
           один x4 PCIe слот для карты расширения, куда можно вставить NVMe x4. Таким
           образом можно сказать что эта плата спроектирована для процессоров с 44 линиями
           и позволяют строить SLI x16 конфигурации + VROC 3x RAID-0.</p>


        <figure><img style="padding: 20px" src="img/RAMPAGE VI APEX.svg?v=2" height=300></figure>

        <p><b>RAMPAGE VI EXTREME Q3 2017</b></p>

        <p>На мой взгляд переусложненная конфигурация с большим количеством PCIe свичей,
           но с дополнительной опцией в виде U.2 порта (правда придется пожертвовать одним
           диском в VROC массиве), и дополнительного M.2 сокета на X299 чипсете, который
           обычно используется для загрузочных дисков.</p>

        <figure><img style="padding: 20px" src="img/RAMPAGE VI EXTREME.svg?v=2" height=300></figure>

        <p><b>RAMPAGE VI EXTREME OMEGA Q1 2019</b></p>

        <p>Омега вернула былую простоту разводки APEX, так же есть M.2 слот на X299 и U.2 слот,
           который так же конфликтует с 3x VROC массивом, но при этом не с DIMM с еще одним дополнительным M.2
           разеведенный на процессор. Второй M.2 конфликтует с PCIe x4 слотом.</p>

        <figure><img style="padding: 20px" src="img/RAMPAGE VI EXTREME OMEGA.svg?v=1" height=300></figure>

        <p><b>RAMPAGE VI EXTREME ENCORE Q3 2019</b></p>

        <p>Последняя ревизия платы без U.2 слота, с обеими M.2 дисками разведенными на X299.
           Плата сохранила традиционную для Rampage VI емкость 32 полный PCIe линий
           и возможность строительства VROC массива на 3 диска.</p>

        <figure><img style="padding: 20px" src="img/RAMPAGE VI EXTREME ENCORE.svg?v=1" height=300></figure>

        <br><br>

       <hr>

       <p>
       [1]. <a href="https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/8th-gen-core-family-datasheet-vol-1.pdf">Intel® Core™ 8th, 9th and Intel® Xeon®E Processor Families #1</a><br>
       [2]. <a href="https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/8th-gen-core-family-datasheet-vol-2.pdf">Intel® Core™ 8th, 9th and Intel® Xeon®E Processor Families #2</a><br>
       [3]. <a href="https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/200-series-chipset-pch-datasheet-vol-1.pdf">Intel® 200 Series: B250, Q250, H270, Q270, Z270, X299, Z370, H310, B365 PCHs #1</a><br>
       [4]. <a href="https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/200-series-chipset-pch-datasheet-vol-2.pdf">Intel® 200 Series: B250, Q250, H270, Q270, Z270, X299, Z370, H310, B365 PCHs #2</a><br>
       [5]. <a href="https://www.intel.com/content/dam/www/public/us/en/documents/technical-specifications/300-series-chipset-on-package-pch-datasheet-vol-1.pdf">Intel® 300 Series: H310, B365, B360, B370, Q370, Z370, Z390 PCHs #1</a><br>
       [6]. <a href="https://www.intel.com/content/dam/www/public/us/en/documents/technical-specifications/300-series-chipset-on-package-pch-datasheet-vol-2.pdf">Intel® 300 Series: H310, B365, B360, B370, Q370, Z370, Z390 PCHs #2</a><br>
       </P>

       <h4>Видеокарты</h4>

       <p>Что касается экспериментов с системами индексации, которые располагаются
          в памяти GPU, то здесь имеет смысл играться только с памятью от 10ГБ, т.е. не ниже 1080 Ti.
          Все карточки, что имеют меньше памяти годятся только для игрушек и то не больших.</p>

       <figure><table cellspacing=10>
       <thead>
       <th>Model</th><th>PCIe</th><th>Chip</th><th>Memory</th><th>Process</th><th>Transistors</th>
       </thead>
       <tbody>
       <!--tr><td>8800 GTX</td><td>1.0</td><td>G80</td><td>768MB DDR3</td><td>90nm</td><td>681M</td></tr>
       <tr><td>8800 GT</td><td>2.0</td><td>G92</td><td>1GB DDR3</td><td>65nm</td><td>754M</td></tr>
       <tr><td>GTX 280</td><td>2.0</td><td>GT200</td><td>1GB DDR3</td><td>65nm</td><td>1.4B</td></tr>
       <tr><td>GTX 340</td><td>2.0</td><td>GT215</td><td>1GB DDR5</td><td>40nm</td><td>727M</td></tr>
       <tr><td>GTX 480</td><td>2.0</td><td>GF100</td><td>1.5GB DDR5</td><td>40nm</td><td>3B</td></tr>
       <tr><td>GTX 580</td><td>2.0</td><td>GF110</td><td>3GB DDR5</td><td>40nm</td><td>3B</td></tr>
       <tr><td>GTX 680</td><td>3.0</td><td>GK104</td><td>4GB DDR5</td><td>28nm</td><td>3540M</td></tr-->
       <tr><td>GTX 780 Ti</td><td>3.0</td><td>GK110</td><td>3GB DDR5</td><td>28nm</td><td>7080M</td></tr>
       <tr><td>GTX 980 Ti</td><td>3.0</td><td>GM200</td><td>6GB DDR5</td><td>28nm</td><td>8B</td></tr>
       <tr style="font-weight: bold;"><td>GTX 1080 Ti</td><td>3.0</td><td>GP102</td><td>11GB DDR5</td><td>16nm</td><td>12B</td></tr>
       <tr><td>RTX 2080 Ti</td><td>3.0</td><td>TU102</td><td>11GB DDR6</td><td>12nm</td><td>18.6B</td></tr>
       <tr style="font-weight: bold;"><td>RTX 3090</td><td>4.0</td><td>GA102</td><td>24GB DDR6</td><td>8nm</td><td>28B</td></tr>
       </tbody></table></figure>

       <p>Видеокарты это второй вид вычислительных мощностей после CPU.
          Современные компьютеры это аналог таких систем как BeBox или Cell/BE, которые
          могут совмещать вычислительные CISC, RISC, SIMD или VLIW блоки.
          Процессоры видеоркарт называются GPU и, как и обычные процессоры CPU,
          имеют свой набор инструктий и свою локальную память.
          Аналог инструкции MOV выступает команда копирования в/из DMA буфера в DDR памяти по PCIe.
          Как и в CPU, в GPU есть ALU инструкции (сложение, умножение) и инструкции
          управления потоками выполнения.</p>

       <p><b>CUDA и RDNA</b></p>

       <p>Главные ядра CUDA [1] называются потоковыми процессорами (SM),
          их количество определяет производительность. Каждый SM состоит из многих скалярных
          процессоров, каждый из которых можно выделить для обработки вершин,
          фиугр, или пикселей. Для управления скалярными процессорами как потоками выполнения
          используется высокоуровневая модель паралельного программирования по типу SIMD,
          только источники и цели операций это не ячейки памяти, а потоки выполнения &mdash; SIMT.</p>

       <p>В картах AMD используется другой набор инструкций [2], но все очень похоже.
          Буфера памяти разделяются на три типа: вершины, фигуры и пиксели.
          Команды GPU &mdash; это вычисления которые используют один тип памяти как источник,
          и другой тип памяти как цель команды. Вершинные шейдеры производят вычисления на вершинах
          и записывают результат в кольцевой буфер примитивов (массивы фигур).
          Геометрические шейдеры производят вычисления на буферах геометрических
          примитивов и записывают результат в память фрагментов, которые представляют
          собор растеризированый кусок экрана в буфере. Фрагмен становится пикселем
          когда переносится в реальный буфер экрана (пиксельные шейдеры).
          На каждом этапе такого пайп лайна вычислительные ядра пользуются
          текстурами, которые уже загружены в память GPU.</p>

       <hr>

       <p>
       [1]. <a href="https://docs.nvidia.com/pdf/ptx_isa_7.0.pdf">PTX ISA 7.0 (2020)</a><br>
       [2]. <a href="https://developer.amd.com/wp-content/resources/RDNA_Shader_ISA.pdf">RDNA ISA 1.0 (2020)</a><br>
       [3]. <a href="https://www.cs.cmu.edu/afs/cs/academic/class/15462-f11/www/lec_slides/lec19.pdf">CMU CS 15-462 Lecture 19 (2011)</a>
       </P>

       <h4>Дисковые массивы</h4>

       <p>Для экспериментов в рамках проекта RocksDB/NVMe нам понадобятся карты
          PCIe x16 (4 слота M.2 каждый по x4 линии),
          для версий PCIe 3.0 и 4.0. Каждая такая карта конфигурируется
          как RAID-0 массив, и таким образом увеличивается скорость
          передачи линейно в 4 раза. Нам нужны именно RAID-0 массивы,
          потому что производительность &mdash; это главная мотивация проекта RocksDB/NVMe.
          Конечно таких карты это нужно вставлять в PCIe слот,
          подключенный напрямую к корневому комплексу, а не к проприетарному мосту
          для подключения периферии. Обычно половина слотов (а на mITX иногда
          и все) M.2 расположеных на материнских платах платформы Intel,
          подключены именно по такой шине DMI, которая имеет пропускную
          способность всего 4 PCIe линии, поэтому RAID-0 на таких слотах строить смысла нет.
          В случае ATX и EATX плат для построение RAID-0 лучше всего
          вставлять платы расширения для M.2 массивов в PCIe x16 слоты
          подключенные к корневому комплексу. В случае mITX систем,
          придется вытаскивать на время GPU и вставлять M.2 RAID-0
          в единственный x16 слот.</p>

       <p>M.2 слоты подключенные к проприетарному мосту типа DMI,
          можно использовать для единичных x4 NVMe инстансов, как загрузочные диски
          для альтернативных ОС и т.д. Я планирую использовать Haiku,
          FreeBSD, Linux, Windows. Не пропадать же M.2 слотам.</p>

       <p>Есть еще вариант использования RocksDB/SATA, строим массив на дисках,
          максимальная пропускная способность которых всего 600МБ/c.
          Для этого на mITX системах можно построить массив из 3-4 дисков,
          а для больших ATX/EATX систем я советую покупать материнские платы,
          где есть 10 SATA портов.</p>

       <p>Эти 10 портов обычно используют для RAID систем, но не аппаратных
          (потому, что восстанавливать любой RAID это нетривиальная задача),
          а программных комплексов типа Unraid, Ceph, GlusterFS. Ceph кстати использует
          RocksDB/NVMe бекенд для одного из типов своих хранилищ BlueStore). Эти системы
          предлагают более удобный нежели аппаратный способ работы с массивами:
          если один диск полетел, вы просто его вытаскиваете и вставляете новый.</p>

       <figure>
         <img src="img/QS-1.jpg" height=200>
         <img src="img/QS-2.jpg" height=200>
         <img src="img/QS-3.jpg" height=200>
       </figure>

       <p>Никаких причин (кроме бедности) использовать механические
          накопители в современном мире нет. Если вам сказали, что твердотельные накопители
          не стоит использовать для бекапа. то это действительно правда, но вместо
          механических накопителей обычно использовали магнитные ленты или магнитооптику,
          которая до сих пор является самым актуальным способом хранения данных, как например
          <a href="https://pro.sony/en_GB/products/stand-alone-drives/ods-d380u">SONY PRO ODS-D380U</a>.</p>

       <p>За 10 лет цена на SSD снизилась с $1 за 1ГБ до $1 за 10ГБ.
          Твердотельные накопители NVMe состоят из DRAM кеша, SLC кеша и
          обычно 64-слойных TLC или QLC чипов. Некоторые проивзодители
          (ADATA, Transcend) используют технологию HMB и поставляют
          диски без DRAM кеша, вместо этого предполагается использование
          памяти хоста, которая подключена к PCIe. Такие диски
          не работают в Thunderbolt переходниках USB-to-PCIe. Нужно покупать только
          те NVMe которые предлагают показателти производительности близкие к
          теоретическому пределу PCIe 3.0 x4 (3938.5МБ/с), и максимально
          не зависят от обьема передаваемых данных (SLC кеш),
          DRAM кеш на 1Т обычно составляет 1ГБ.
          В 2020 году покупать NVMe меньше 1Т смысла нет, они быстрее, в них больше SLC кеша,
          более качественные микросхемы, выгодная удельная стоимость единицы объема.</p>

       <figure><table cellspacing=10>
       <thead>
       <th>Model</th><th>PCIe</th><th>Size</th><th>Chip</th><th>Flash</th><th>MB/s</th><th>Lifetime</th>
       </thead>
       <tbody>
       <tr style="font-weight: bold;"><td>Samsung&nbsp;970&nbsp;EVO+</td><td>3.0&nbsp;x4</td><td>1T</td><td>Samsung&nbsp;Phoenix</td><td>64&nbsp;TLC&nbsp;Samsung&nbsp;256G</td><td>3500/3300</td><td>600T</td></tr>
       <tr style="font-weight: bold;"><td>Samsung&nbsp;970&nbsp;PRO</td><td>3.0&nbsp;x4</td><td>1T</td><td>Samsung&nbsp;Phoenix</td><td>64&nbsp;MLC&nbsp;Samsung&nbsp;256G</td><td>3500/3300</td><td>1200T</td></tr>
       <tr><td>WS Black SN750</td><td>3.0&nbsp;x4</td><td>1T</td><td>SanDisk</td><td>64 TLC SanDisk 256G</td><td>3470/3000</td><td>600T</td></tr>
       <tr><td>Patriot VPN100</td><td>3.0&nbsp;x4</td><td>1T</td><td>Phison PS5012-E12</td><td>64 TLC Toshiba 256G</td><td>3450/3000</td><td>1665T</td></tr>
       <tr><td>Intel 760P</td><td>3.0&nbsp;x4</td><td>1T</td><td>SMI SM2262</td><td>64 TLC Intel 256G</td><td>3230/1625</td><td>576T</td></tr>
       <tr><td>ATADA XPG S11 Pro</td><td>3.0&nbsp;x4</td><td>1T</td><td>SMI SM2262EN</td><td>64 TLC Micron 512G</td><td>3500/3000</td><td>640T</td></tr>
       <tr><td>Transcend MTE220S</td><td>3.0&nbsp;x4</td><td>1T</td><td>SMI SM2262EN</td><td>64 TLC Micron 512G</td><td>3500/2800</td><td>640T</td></tr>
       <tr><td>Kingston KC2000</td><td>3.0&nbsp;x4</td><td>1T</td><td>SMI SM2262EN</td><td>96 TLC Toshiba 512G</td><td>3200/2200</td><td>600T</td></tr>
       <tr><td>Sabrent Rocket</td><td>4.0&nbsp;x4</td><td>1T</td><td>Phison PS5016-E16</td><td>96 TLC Toshiba 512G</td><td>5000/4400</td><td>1800T</td></tr>
       <tr><td>Corsair Force MP600</td><td>4.0&nbsp;x4</td><td>1T</td><td>Phison PS5016-E16</td><td>96 TLC Toshiba 512G</td><td>4950/2500</td><td>1800Т</td></tr>
       <tr style="font-weight: bold;"><td>FireCUDA&nbsp;520</td><td>4.0&nbsp;x4</td><td>1T</td><td><nobr>Phison PS5016-E16</nobr></td><td>96 TLC Toshiba 512G</td><td>5000/4400</td><td>1800Т</td></tr>
       <tr><td>ADATA GAMMIX S50</td><td>4.0&nbsp;x4</td><td>1T</td><td>Phison PS5016-E16</td><td>96 TLC Toshiba 512G</td><td>5000/4400</td><td>1800Т</td></tr>
       </tbody></table></figure>

       <p>Коллекционными считаются четырехканальные RAID-0 переходники
          на 16 линий PCIe, т.е. четыре диска (если меньше &mdash; не берите).
          Если на материнской плате остаются свободные х8 слоты и хочется их использовать для RAID,
          то можно использовать x8 M.2 адаптеры на два диска.</p>

       <figure><table cellspacing=10>
       <thead>
       <th>Model</th><th>PCIe</th>
       </thead>
       <tbody>
       <tr style="font-weight: bold;"><td>ASUS Hyper M.2 V2</td><td>3.0&nbsp;x16</td></tr>
       <tr style="font-weight: bold;"><td>ASUS Hyper M.2</td><td>4.0&nbsp;x16</td></tr>
       <tr><td>ASRock ULTRA QUAD</td><td>3.0&nbsp;x16</td></tr>
       <tr><td>AORUS Gen4 AIC</td><td>4.0&nbsp;x16</td></tr>
       <tr><td>Startech x8 Dual M.2</td><td>3.0&nbsp;x8</td></tr>
       <tr><td>Sonnet M.2 4x4</td><td>3.0&nbsp;x8</td></tr>
       <tr><td>Synology M2D18</td><td>4.0&nbsp;x8</td></tr>
       </tbody></table></figure>

       <hr>

       <p>
       [1]. <a href="https://spdk.io/doc/">SPDK.IO/DOC</a><br>
       [2]. <a href="https://nvmexpress.org/wp-content/uploads/NVM-Express-1_4-2019.06.10-Ratified.pdf">NVMe 1.4</a><br>
       [3]. <a href="https://github.com/Beyer-Yan/kvstore">Yan Beyer. A fast kvstore based on spdk programing framework.</a><br>
       [4]. <a href="https://www.usenix.org/system/files/hotstorage19-paper-yoshimura.pdf">T.Yoshimura, T.Chiba, H.Horii. EvFS: User-level, Event-Driven File System for Non-Volatile Memory.</a><br>
       </P>

       <h4>Сеть</h4>

       <p></p>

       <h4>Корпуса</h4>

       <p>Как видно из таблиц, все платы можно разделить на две категории: маленькие и большие.
          В категории малых корпусов мне больше всего понравились DeepCool Tristellar,
          NZXT H200i, AZZA Pyramyd 804. Для полноразмерных больших CEB/EEB/XLATX
          плат &mdash; Lian Li O11D XL, а для EATX плат и дисковых
          массивов &mdash; DeepCool Quadstellar. Всего 5 корпусов. Если хватает
          денег можно еще купить стол Lian Li.</p>

       <h4>Итоговая конфигурация</h4>

       <p>&mdash; (1) GamerStorm DeepCool Quadstellar<br>
          &mdash; (2) Seasonic Platinum 1000W<br>
          &mdash; (3) Intel i9-10980XE 3GHz<br>
          &mdash; (4) G.SKILL 128GB 3600 CL17 8x16GB GTZR<br>
          &mdash; (5) ASUS STRIX LC 360<br>
          &mdash; (6) ASUS RAMPAGE VI EXTREME OMEGA<br>
          &mdash; (7) Samsung 970 EVO+<br>
         </p>

    </section>
</main>
<footer>Namdak Tonpa <span class="heart">&nbsp;❤&nbsp;</span> 2020</footer>
</body>
</html>
